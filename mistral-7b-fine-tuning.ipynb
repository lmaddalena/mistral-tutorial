{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"source":["# Mistral 7B fine-tuning\n","https://www.datacamp.com/tutorial/mistral-7b-tutorial\n","\n","https://www.kaggle.com/code/lmaddalena/mistral-7b-fine-tuning/edit"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:07:13.520717Z","iopub.status.busy":"2024-01-15T08:07:13.520085Z","iopub.status.idle":"2024-01-15T08:08:37.878445Z","shell.execute_reply":"2024-01-15T08:08:37.877207Z","shell.execute_reply.started":"2024-01-15T08:07:13.520674Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting bitsandbytes\n","  Obtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/9b/63/489ef9cd7a33c1f08f1b2be51d1b511883c5e34591aaa9873b30021cd679/bitsandbytes-0.42.0-py3-none-any.whl.metadata\n","  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes) (1.24.3)\n","Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.42.0\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.2)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n","Note: you may need to restart the kernel to use updated packages.\n","Collecting peft\n","  Obtaining dependency information for peft from https://files.pythonhosted.org/packages/8b/1b/aee2a330d050c493642d59ba6af51f3910cb138ea48ede228c84c204a5af/peft-0.7.1-py3-none-any.whl.metadata\n","  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.24.3)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\n","Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.36.2)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\n","Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.1)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.20.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.12.2)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.8.8)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n","\u001b[?25hInstalling collected packages: peft\n","Successfully installed peft-0.7.1\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\n","Collecting accelerate\n","  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/a6/b9/44623bdb05595481107153182e7f4b9f2ef9d3b674938ad13842054dcbd8/accelerate-0.26.1-py3-none-any.whl.metadata\n","  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.12.2)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: accelerate\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.25.0\n","    Uninstalling accelerate-0.25.0:\n","      Successfully uninstalled accelerate-0.25.0\n","Successfully installed accelerate-0.26.1\n","Note: you may need to restart the kernel to use updated packages.\n","Collecting trl\n","  Obtaining dependency information for trl from https://files.pythonhosted.org/packages/e0/77/f3eb5ef929718a66f21c2c6e78df0e85132ee4e9ac169afcaaca458b11f6/trl-0.7.9-py3-none-any.whl.metadata\n","  Downloading trl-0.7.9-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.0.0)\n","Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.36.2)\n","Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.24.3)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.26.1)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.0)\n","Collecting tyro>=0.5.11 (from trl)\n","  Obtaining dependency information for tyro>=0.5.11 from https://files.pythonhosted.org/packages/eb/2b/0cdd7c000d4aeb8f6cf78b9b2fefae2540118870cf1446c87fccacea3b7e/tyro-0.6.4-py3-none-any.whl.metadata\n","  Downloading tyro-0.6.4-py3-none-any.whl.metadata (7.7 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.12.2)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.5.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.20.2)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2023.8.8)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (4.66.1)\n","Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.15)\n","Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.5.2)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n","  Obtaining dependency information for shtab>=1.5.6 from https://files.pythonhosted.org/packages/40/ad/7227da64498eaa7abecee4311008f70869e156014b3270cec36e2e70cd31/shtab-1.6.5-py3-none-any.whl.metadata\n","  Downloading shtab-1.6.5-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (11.0.0)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.7)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.0.3)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2023.12.2)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.8.5)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.18.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl) (3.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (2023.11.17)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n","Downloading trl-0.7.9-py3-none-any.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading tyro-0.6.4-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.9/78.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.6.5-py3-none-any.whl (13 kB)\n","Installing collected packages: shtab, tyro, trl\n","Successfully installed shtab-1.6.5 trl-0.7.9 tyro-0.6.4\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n","Collecting datasets\n","  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/ec/93/454ada0d1b289a0f4a86ac88dbdeab54921becabac45da3da787d136628f/datasets-2.16.1-py3-none-any.whl.metadata\n","  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.12.2)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\n","Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n","Collecting pyarrow-hotfix (from datasets)\n","  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n","Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets)\n","  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n","  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n","\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyarrow-hotfix, fsspec, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2023.12.2\n","    Uninstalling fsspec-2023.12.2:\n","      Successfully uninstalled fsspec-2023.12.2\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.1.0\n","    Uninstalling datasets-2.1.0:\n","      Successfully uninstalled datasets-2.1.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n","cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\n","cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\n","dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\n","dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\n","dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n","dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\n","dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\n","dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n","gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\n","raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\n","raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\n","s3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.16.1 fsspec-2023.10.0 pyarrow-hotfix-0.6\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# Update and install the necessary Python libraries\n","\n","%pip install -U bitsandbytes\n","%pip install -U transformers\n","%pip install -U peft\n","%pip install -U accelerate\n","%pip install -U trl\n","%pip install -U datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:08:37.881152Z","iopub.status.busy":"2024-01-15T08:08:37.880736Z","iopub.status.idle":"2024-01-15T08:09:06.143813Z","shell.execute_reply":"2024-01-15T08:09:06.143024Z","shell.execute_reply.started":"2024-01-15T08:08:37.881109Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["# Load the necessary modules for effective fine-tuning of the model.\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n","from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n","import os,torch, wandb\n","from datasets import load_dataset\n","from trl import SFTTrainer"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:09:06.145564Z","iopub.status.busy":"2024-01-15T08:09:06.144965Z","iopub.status.idle":"2024-01-15T08:09:06.503499Z","shell.execute_reply":"2024-01-15T08:09:06.502407Z","shell.execute_reply.started":"2024-01-15T08:09:06.145536Z"},"trusted":true},"outputs":[],"source":["# Save Hugging Face and Weights and Biases API keys and access them in the Kaggle notebook.\n","\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","secret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n","secret_wandb = user_secrets.get_secret(\"wandb\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:09:06.507069Z","iopub.status.busy":"2024-01-15T08:09:06.506642Z","iopub.status.idle":"2024-01-15T08:09:08.065488Z","shell.execute_reply":"2024-01-15T08:09:08.064552Z","shell.execute_reply.started":"2024-01-15T08:09:06.507030Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["# Use the Hugging Face API to save and push the model to the Hugging Face Hub\n","\n","!huggingface-cli login --token $secret_hf"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:09:08.067139Z","iopub.status.busy":"2024-01-15T08:09:08.066827Z","iopub.status.idle":"2024-01-15T08:09:40.977392Z","shell.execute_reply":"2024-01-15T08:09:40.976454Z","shell.execute_reply.started":"2024-01-15T08:09:08.067109Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlmaddalena\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.16.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240115_080910-voz27lqp</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/lmaddalena/Fine%20tuning%20mistral%207B/runs/voz27lqp' target=\"_blank\">iconic-snowball-4</a></strong> to <a href='https://wandb.ai/lmaddalena/Fine%20tuning%20mistral%207B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/lmaddalena/Fine%20tuning%20mistral%207B' target=\"_blank\">https://wandb.ai/lmaddalena/Fine%20tuning%20mistral%207B</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/lmaddalena/Fine%20tuning%20mistral%207B/runs/voz27lqp' target=\"_blank\">https://wandb.ai/lmaddalena/Fine%20tuning%20mistral%207B/runs/voz27lqp</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# To monitor LLM performance, we will initialize Weights and Biases experiments using API.\n","\n","wandb.login(key = secret_wandb)\n","run = wandb.init(\n","    project='Fine tuning mistral 7B', \n","    job_type=\"training\", \n","    anonymous=\"allow\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["In this section, we will set the base model, dataset, and new model name. The name of the new model will be used to save a fine-tuned model.\n","\n","Note: If you are using the free version of Colab, you should load the sharded version of the model (someone13574/Mistral-7B-v0.1-sharded).\n","\n","You can also load the model from Hugging Face Hub using the base model name: mistralai/Mistral-7B-v0.1"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:09:40.979501Z","iopub.status.busy":"2024-01-15T08:09:40.978856Z","iopub.status.idle":"2024-01-15T08:09:40.989984Z","shell.execute_reply":"2024-01-15T08:09:40.988948Z","shell.execute_reply.started":"2024-01-15T08:09:40.979467Z"},"trusted":true},"outputs":[],"source":["base_model = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\n","dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","new_model = \"mistral_7b_guanaco\""]},{"cell_type":"markdown","metadata":{},"source":["### Data Loading"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:09:40.992318Z","iopub.status.busy":"2024-01-15T08:09:40.991775Z","iopub.status.idle":"2024-01-15T08:09:43.452867Z","shell.execute_reply":"2024-01-15T08:09:43.452014Z","shell.execute_reply.started":"2024-01-15T08:09:40.992287Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec64ffb565f24fabba54f93a992249bf","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81c3445048f74fabbd8de4027c15197a","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/967k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73d3a2b0b4d04aa98af0d2469be6cb6e","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["'<s>[INST] cuanto es 2x2 xD [/INST] La respuesta es 4. </s><s>[INST] puedes demostrarme matematicamente que 2x2 es 4? [/INST] En una multiplicación, el producto es el resultado de sumar un factor tantas veces como indique el otro, es decir, si tenemos una operación v · n = x, entonces x será igual a v sumado n veces o n sumado v veces, por ejemplo, para la multiplicación 3 · 4 podemos sumar \"3 + 3 + 3 + 3\" o \"4 + 4 + 4\" y en ambos casos nos daría como resultado 12, para el caso de 2 · 2 al ser iguales los dos factores el producto sería \"2 + 2\" que es igual a 4 </s>'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Load the dataset from Hugging Face Hub and visualize the 100th row.\n","\n","#Importing the dataset\n","dataset = load_dataset(dataset_name, split=\"train\")\n","dataset[\"text\"][100]"]},{"cell_type":"markdown","metadata":{},"source":["### Loading the Mistral 7B model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:09:43.454692Z","iopub.status.busy":"2024-01-15T08:09:43.454304Z","iopub.status.idle":"2024-01-15T08:12:46.654168Z","shell.execute_reply":"2024-01-15T08:12:46.652832Z","shell.execute_reply.started":"2024-01-15T08:09:43.454657Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f6d3688d5b14f59b6a944cd9a33f7a4","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["bnb_config = BitsAndBytesConfig(  \n","    load_in_4bit= True,\n","    bnb_4bit_quant_type= \"nf4\",\n","    bnb_4bit_compute_dtype= torch.bfloat16,\n","    bnb_4bit_use_double_quant= False,\n",")\n","model = AutoModelForCausalLM.from_pretrained(\n","        base_model,\n","        load_in_4bit=True,\n","        quantization_config=bnb_config,\n","        torch_dtype=torch.bfloat16,\n","        device_map=\"auto\",\n","        trust_remote_code=True,\n",")\n","model.config.use_cache = False # silence the warnings\n","model.config.pretraining_tp = 1\n","model.gradient_checkpointing_enable()"]},{"cell_type":"markdown","metadata":{},"source":["### Loading the Tokenizer"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:12:46.656028Z","iopub.status.busy":"2024-01-15T08:12:46.655670Z","iopub.status.idle":"2024-01-15T08:12:46.821799Z","shell.execute_reply":"2024-01-15T08:12:46.820510Z","shell.execute_reply.started":"2024-01-15T08:12:46.655989Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(True, True)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n","tokenizer.padding_side = 'right'\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.add_eos_token = True\n","tokenizer.add_bos_token, tokenizer.add_eos_token"]},{"cell_type":"markdown","metadata":{},"source":["### Adding the adopter to the layer\n","In the next step, we will include an adopter layer in our model. This will enable us to fine-tune the model using a small number of parameters, making the entire process faster and more memory-efficient. To gain a better understanding of parameters, you can refer to the official documentation of PEFT."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:12:46.827036Z","iopub.status.busy":"2024-01-15T08:12:46.826710Z","iopub.status.idle":"2024-01-15T08:12:48.183516Z","shell.execute_reply":"2024-01-15T08:12:48.182235Z","shell.execute_reply.started":"2024-01-15T08:12:46.827003Z"},"trusted":true},"outputs":[],"source":["model = prepare_model_for_kbit_training(model)\n","peft_config = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.1,\n","    r=64,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",")\n","model = get_peft_model(model, peft_config)"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparmeters"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:12:48.185629Z","iopub.status.busy":"2024-01-15T08:12:48.184828Z","iopub.status.idle":"2024-01-15T08:12:48.197294Z","shell.execute_reply":"2024-01-15T08:12:48.195930Z","shell.execute_reply.started":"2024-01-15T08:12:48.185581Z"},"trusted":true},"outputs":[],"source":["training_arguments = TrainingArguments(\n","    output_dir=\"./results\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=1,\n","    optim=\"paged_adamw_32bit\",\n","    save_steps=25,\n","    logging_steps=25,\n","    learning_rate=2e-4,\n","    weight_decay=0.001,\n","    fp16=False,\n","    bf16=False,\n","    max_grad_norm=0.3,\n","    max_steps=-1,\n","    warmup_ratio=0.03,\n","    group_by_length=True,\n","    lr_scheduler_type=\"constant\",\n","    report_to=\"wandb\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### SFT parameters\n","HuggingFace's TRL library offers a user-friendly API that allows for the creation and training of Supervised fine-tuning (SFT) models on your dataset with minimal coding. We will provide the SFT Trainer with the necessary components, such as the model, dataset, Lora configuration, tokenizer, and training parameters."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:12:48.199404Z","iopub.status.busy":"2024-01-15T08:12:48.198935Z","iopub.status.idle":"2024-01-15T08:12:49.051073Z","shell.execute_reply":"2024-01-15T08:12:49.049983Z","shell.execute_reply.started":"2024-01-15T08:12:48.199343Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:222: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35a5e20b33b04b9bbaef83fbe886200f","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    max_seq_length= None,\n","    dataset_text_field=\"text\",\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing= False,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Model training"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:12:49.053788Z","iopub.status.busy":"2024-01-15T08:12:49.052907Z","iopub.status.idle":"2024-01-15T09:53:36.755535Z","shell.execute_reply":"2024-01-15T09:53:36.754495Z","shell.execute_reply.started":"2024-01-15T08:12:49.053744Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [250/250 1:39:51, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>1.034600</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.663700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>1.074100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.353200</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>1.050400</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.252700</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>1.051300</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.293700</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>1.003800</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.338300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=250, training_loss=1.2115845413208008, metrics={'train_runtime': 6046.5916, 'train_samples_per_second': 0.165, 'train_steps_per_second': 0.041, 'total_flos': 1.874641569231667e+16, 'train_loss': 1.2115845413208008, 'epoch': 1.0})"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["### Saving the fine-tuned model"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T09:53:36.757094Z","iopub.status.busy":"2024-01-15T09:53:36.756782Z","iopub.status.idle":"2024-01-15T09:53:40.815567Z","shell.execute_reply":"2024-01-15T09:53:40.814838Z","shell.execute_reply.started":"2024-01-15T09:53:36.757067Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▁█▂▅▁▄▂▄▁▅</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>1.3383</td></tr><tr><td>train/total_flos</td><td>1.874641569231667e+16</td></tr><tr><td>train/train_loss</td><td>1.21158</td></tr><tr><td>train/train_runtime</td><td>6046.5916</td></tr><tr><td>train/train_samples_per_second</td><td>0.165</td></tr><tr><td>train/train_steps_per_second</td><td>0.041</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">iconic-snowball-4</strong> at: <a href='https://wandb.ai/lmaddalena/Fine%20tuning%20mistral%207B/runs/voz27lqp' target=\"_blank\">https://wandb.ai/lmaddalena/Fine%20tuning%20mistral%207B/runs/voz27lqp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240115_080910-voz27lqp/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["trainer.model.save_pretrained(new_model)\n","wandb.finish()\n","model.config.use_cache = True"]},{"cell_type":"markdown","metadata":{},"source":["We can easily upload our model to the Hugging Face Hub with a single line of code, allowing us to access it from any machine."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#trainer.model.push_to_hub(new_model,use_temp_dir=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Model evaluation\n","You can view system metrics and model performance by going to wandb.ai and checking the recent run."]},{"cell_type":"markdown","metadata":{},"source":["## Inference\n","To perform model inference, we need to provide both the model and tokenizer objects to the pipeline. Then, we can provide the prompt in dataset style to the pipeline object."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T09:56:36.910617Z","iopub.status.busy":"2024-01-15T09:56:36.909784Z","iopub.status.idle":"2024-01-15T09:59:27.854930Z","shell.execute_reply":"2024-01-15T09:59:27.853874Z","shell.execute_reply.started":"2024-01-15T09:56:36.910579Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"]},{"name":"stdout","output_type":"stream","text":["<s>[INST] How do I find true love? [/INST] There is no one way to find true love. It is a complex and personal journey that can take many different paths. Some people find love through friends, family, or chance encounters. Others may find love through online dating or other forms of matchmaking. Ultimately, the best way to find true love is to be open to new experiences and to be willing to put yourself out there. \n","\n","If you are looking for more specific advice, I can try to help. What are you looking for in a partner? What are your goals for a relationship? What are your values and beliefs? What are your interests and hobbies? What are your dealbreakers? What are your dealmakers? What are your dealbreakers? What are your dealmakers? What are your dealbreakers? What are your dealmakers? What are your dealbreakers? What are your dealmakers? What are your\n"]}],"source":["logging.set_verbosity(logging.CRITICAL)\n","\n","prompt = \"How do I find true love?\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"markdown","metadata":{},"source":["Let’s generate the response for another prompt."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-01-15T09:59:58.293858Z","iopub.status.busy":"2024-01-15T09:59:58.293109Z","iopub.status.idle":"2024-01-15T10:02:47.197468Z","shell.execute_reply":"2024-01-15T10:02:47.196484Z","shell.execute_reply.started":"2024-01-15T09:59:58.293820Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s>[INST] What is Datacamp Career track? [/INST] Datacamp Career track is a program that helps you learn data science and data engineering skills. It provides you with a personalized learning path, projects, and mentorship to help you advance your career. \n","\n","You can learn more about it here: https://www.datacamp.com/career-track \n","\n","Does this answer your question? 😊 \n","\n","If you have any other questions, feel free to ask! 😊 \n","\n","I'm here to help! 😊 \n","\n","If you have any other questions, feel free to ask! 😊 \n","\n","I'm here to help! 😊 \n","\n","If you have any other questions, feel free to ask! 😊 \n","\n","I'm here to help! 😊 \n","\n","If you have any other questions, feel free to ask! 😊 \n","\n","\n"]}],"source":["prompt = \"What is Datacamp Career track?\"\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"isSourceIdPinned":true,"modelInstanceId":3899,"sourceId":5111,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
